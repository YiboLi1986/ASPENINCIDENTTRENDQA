import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))

from pathlib import Path
from typing import List, Dict, Optional
import pandas as pd
import numpy as np

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy import sparse
import joblib
from rapidfuzz import fuzz

from backend.src.embeddings.embedding_handler import EmbeddingHandler


class IncidentSearcher:
    """
    TF-IDF + Fuzzy search with optional Embedding rerank over incidents.csv.

    Requires artifacts generated by IncidentIndexer:
      - src/data/processed/incidents.csv
      - src/data/processed/index/vectorizer.pkl
      - src/data/processed/index/tfidf_csr.npz
      - src/data/processed/index/mapping.csv

    (Optional) Requires artifacts generated by IncidentEmbedder (for embedding rerank):
      - src/data/processed/embeddings/embeddings.npy
      - src/data/processed/embeddings/kept_indices.npy
      - src/data/processed/embeddings/embedder_meta.json
    """

    def __init__(
        self,
        project_root: Optional[str] = None,
        processed_subdir: str = "src/data/processed",
        index_subdir: str = "src/data/processed/index",
        emb_subdir: str = "src/data/processed/embeddings",
        # stage-1 blend weight: final_stage1 = alpha * tfidf_cos + (1-alpha) * fuzzy
        alpha: float = 0.8,
        # stage-2 rerank weight: final = (1-beta) * final_stage1 + beta * embed_cos
        beta: float = 0.25,
        # how many top results from stage-1 enter embedding rerank
        candidate_pool: int = 200,
        # embedding runtime config
        embed_model_name: str = "nomic-embed-text:latest",
        embed_base_url: Optional[str] = None,
    ) -> None:
        self.project_root = Path(project_root) if project_root else Path(__file__).resolve().parents[2]
        self.proc_dir = (self.project_root / processed_subdir).resolve()
        self.index_dir = (self.project_root / index_subdir).resolve()
        self.emb_dir = (self.project_root / emb_subdir).resolve()

        # indexer artifacts
        self.incidents_csv = self.proc_dir / "incidents.csv"
        self.vectorizer_pkl = self.index_dir / "vectorizer.pkl"
        self.matrix_npz = self.index_dir / "tfidf_csr.npz"
        self.mapping_csv = self.index_dir / "mapping.csv"

        # embedder artifacts (optional)
        self.emb_npy = self.emb_dir / "embeddings.npy"
        self.kept_idx_npy = self.emb_dir / "kept_indices.npy"
        self.meta_json = self.emb_dir / "embedder_meta.json"

        # weights
        self.alpha = float(alpha)
        self.beta = float(beta)
        self.candidate_pool = int(candidate_pool)

        # data holders
        self.df: Optional[pd.DataFrame] = None
        self.vec: Optional[TfidfVectorizer] = None
        self.mat: Optional[sparse.csr_matrix] = None

        # embeddings (optional)
        self.doc_emb: Optional[np.ndarray] = None       # shape (M, D)
        self.kept_indices: Optional[np.ndarray] = None  # shape (M,)
        self.pos_map: Optional[dict] = None             # original_csv_idx -> position in doc_emb
        self.emb_normalized: bool = True                # default true; read from meta if present

        # query embedder
        self.embedder: Optional[EmbeddingHandler] = None
        self.embed_model_name = embed_model_name
        self.embed_base_url = embed_base_url or os.getenv("OLLAMA_HOST") or "http://172.22.5.186:32000/ollama-dev"

        # load everything
        self._load_index_artifacts()
        self._maybe_load_embedding_artifacts()
        self._maybe_init_embedder()

    # ---------------- public ----------------
    def search(
        self,
        query: str,
        top_k: int = 8,
        min_desc_len: int = 0,
        same_resolution_dedupe: bool = True,
    ) -> List[Dict]:
        """
        Stage-1: TF-IDF cosine + Fuzzy blended.
        Stage-2: (Optional) Embedding rerank over the top `candidate_pool` from stage-1.
        """
        if not query or self.df is None or self.vec is None or self.mat is None:
            return []

        # ---------- Stage-1 ----------
        q_vec = self.vec.transform([query])
        tfidf_cos = cosine_similarity(q_vec, self.mat).ravel()

        desc = self.df["description"].astype(str).tolist()
        fuzzy_scores = np.array([fuzz.token_set_ratio(query, d) for d in desc], dtype=float) / 100.0

        stage1 = self.alpha * tfidf_cos + (1.0 - self.alpha) * fuzzy_scores

        # candidate pool for rerank
        pool_n = min(max(self.candidate_pool, top_k), len(stage1))
        pool_idx = np.argpartition(-stage1, pool_n - 1)[:pool_n]
        pool_idx = pool_idx[np.argsort(-stage1[pool_idx])]

        # ---------- Stage-2 (optional embedding rerank) ----------
        final_scores = stage1.copy()

        if self.embedder is not None and self.doc_emb is not None and self.pos_map is not None:
            try:
                q_emb = np.asarray(self.embedder.encode_one(query), dtype=np.float32)
                if self.emb_normalized:
                    # normalize both sides to ensure dot == cosine
                    q_emb = self._l2_normalize(q_emb.reshape(1, -1)).reshape(-1)
                else:
                    # normalize query only; doc_emb may be unnormalized, so compute cosine manually
                    q_emb = self._l2_normalize(q_emb.reshape(1, -1)).reshape(-1)

                embed_scores = np.zeros_like(stage1, dtype=np.float32)

                # compute embedding score only on candidate pool for speed
                for i in pool_idx:
                    pos = self.pos_map.get(int(i))
                    if pos is None:
                        continue  # this original CSV row wasn't embedded (was skipped)
                    d = self.doc_emb[pos]
                    if not self.emb_normalized:
                        d = self._l2_normalize(d.reshape(1, -1)).reshape(-1)
                    embed_scores[i] = float(d @ q_emb)  # cosine/IP

                # blend: final = (1 - beta) * stage1 + beta * embed
                final_scores = (1.0 - self.beta) * stage1 + self.beta * embed_scores
            except Exception:
                # if embedding fails for any reason, silently fallback to stage1 only
                final_scores = stage1

        # ---------- Top-K selection ----------
        k = min(max(top_k, 1), len(final_scores))
        idx = np.argpartition(-final_scores, k - 1)[:k]
        idx = idx[np.argsort(-final_scores[idx])]

        # assemble results
        out: List[Dict] = []
        seen_res = set()
        for i in idx:
            row = self.df.iloc[int(i)]
            if min_desc_len and len(str(row["description"])) < min_desc_len:
                continue

            res_text = str(row["resolution"]).strip()
            if same_resolution_dedupe:
                if res_text in seen_res:
                    continue
                seen_res.add(res_text)

            out.append({
                "id": row.get("id", ""),
                "description": row.get("description", ""),
                "resolution": res_text,
                "source_file": row.get("source_file", ""),
                "score_tfidf_fuzzy": round(float(stage1[i]), 4),
                "score_final": round(float(final_scores[i]), 4),
            })
        return out

    # ---------------- internals ----------------
    def _load_index_artifacts(self) -> None:
        if not self.incidents_csv.exists():
            raise FileNotFoundError(f"incidents.csv not found: {self.incidents_csv}")
        if not self.vectorizer_pkl.exists() or not self.matrix_npz.exists() or not self.mapping_csv.exists():
            raise FileNotFoundError("Index artifacts missing. Run indexer first.")

        self.df = pd.read_csv(self.incidents_csv).fillna("")
        self.vec = joblib.load(self.vectorizer_pkl)
        self.mat = sparse.load_npz(self.matrix_npz)

        n = min(len(self.df), self.mat.shape[0])
        self.df = self.df.iloc[:n].reset_index(drop=True)
        if self.mat.shape[0] != n:
            self.mat = self.mat[:n]

    def _maybe_load_embedding_artifacts(self) -> None:
        if not (self.emb_npy.exists() and self.kept_idx_npy.exists()):
            # embeddings are optional; skip silently
            return
        self.doc_emb = np.load(self.emb_npy).astype(np.float32)
        self.kept_indices = np.load(self.kept_idx_npy).astype(np.int64)
        # build map: original CSV row index -> position in doc_emb
        self.pos_map = {int(orig_idx): int(pos) for pos, orig_idx in enumerate(self.kept_indices.tolist())}

        # read normalize flag
        self.emb_normalized = True
        try:
            import json
            meta = json.loads(Path(self.meta_json).read_text(encoding="utf-8"))
            self.emb_normalized = bool(meta.get("normalize", True))
        except Exception:
            pass

    def _maybe_init_embedder(self) -> None:
        if self.doc_emb is None:
            # no embeddings available; keep None to disable stage-2
            return
        self.embedder = EmbeddingHandler(
            model_name=self.embed_model_name,
            base_url=self.embed_base_url,
        )

    @staticmethod
    def _l2_normalize(x: np.ndarray) -> np.ndarray:
        if x.ndim == 1:
            denom = max(np.linalg.norm(x), 1e-12)
            return (x / denom).astype(np.float32)
        denom = np.clip(np.linalg.norm(x, axis=1, keepdims=True), 1e-12, None)
        return (x / denom).astype(np.float32)


if __name__ == "__main__":
    s = IncidentSearcher(alpha=0.8, beta=0.25, candidate_pool=200)
    while True:
        try:
            q = input("\nQuery (blank to exit): ").strip()
            if not q:
                break
            results = s.search(q, top_k=8, min_desc_len=10, same_resolution_dedupe=True)
            for i, r in enumerate(results, 1):
                print(f"\n[{i}] score_final={r['score_final']}  file={r['source_file']}")
                print(f"Q : {q}")
                print(f"D : {r['description'][:300]}")
                print(f"R : {r['resolution'][:300]}")
        except KeyboardInterrupt:
            break
